# ğŸ” Enterprise RAG System

Production-ready Retrieval-Augmented Generation system with guardrails, built with Python.

![Python](https://img.shields.io/badge/Python-3.12-blue)
![Tests](https://img.shields.io/badge/Tests-172%20passing-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## âœ¨ Features

- **Modular Architecture** - Pluggable loaders, chunkers, embeddings, vectorstores
- **Production Guardrails** - Score thresholds, source validation, confidence levels
- **Multiple Interfaces** - REST API (FastAPI) + Web UI (Streamlit)
- **RAGAS-style Evaluation** - Faithfulness, relevance, context precision metrics
- **Docker Ready** - One-command deployment with GPU support

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Loaders   â”‚â”€â”€â”€â”€â–¶â”‚  Chunkers   â”‚â”€â”€â”€â”€â–¶â”‚ Embeddings  â”‚
â”‚ PDF/TXT/MD  â”‚     â”‚Fixed/Recurs â”‚     â”‚   Ollama    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LLM     â”‚â—€â”€â”€â”€â”€â”‚  Retrieval  â”‚â—€â”€â”€â”€â”€â”‚ VectorStore â”‚
â”‚   Ollama    â”‚     â”‚    Dense    â”‚     â”‚   Qdrant    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Guardrails  â”‚â”€â”€â–¶ Confidence: ğŸŸ¢ HIGH | ğŸŸ¡ MEDIUM | ğŸ”´ LOW
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.12+
- Docker & Docker Compose
- Ollama installed locally

### Option 1: Docker (Recommended)
```bash
# GPU systems
docker-compose up -d

# CPU only
docker-compose -f docker-compose.cpu.yml up -d

# Pull required models
docker exec rag-ollama ollama pull llama3.2:latest
docker exec rag-ollama ollama pull nomic-embed-text
```

Access:
- API: http://localhost:8000
- UI: http://localhost:8501
- API Docs: http://localhost:8000/docs

### Option 2: Local Development
```bash
# Clone repository
git clone https://github.com/yourusername/rag-system.git
cd rag-system

# Create virtual environment
python -m venv rag-env
source rag-env/bin/activate  # Windows: rag-env\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start services
docker run -d --name qdrant -p 6333:6333 qdrant/qdrant
ollama serve &
ollama pull llama3.2:latest
ollama pull nomic-embed-text

# Run API
uvicorn src.api.main:app --reload

# Or run UI
streamlit run src/ui/app.py
```

## ğŸ“– Usage

### Python SDK
```python
from src.pipeline import ProductionRAGPipeline
from src.embeddings import OllamaEmbeddings
from src.vectorstores import QdrantVectorStore
from src.retrieval import DenseRetriever
from src.generation import OllamaLLM
from src.guardrails import GuardrailsConfig

# Initialize components
embeddings = OllamaEmbeddings(model="nomic-embed-text")
vectorstore = QdrantVectorStore(collection_name="my_docs")
retriever = DenseRetriever(embeddings=embeddings, vectorstore=vectorstore)
llm = OllamaLLM(model="llama3.2:latest")

# Create pipeline with guardrails
pipeline = ProductionRAGPipeline(
    embeddings=embeddings,
    vectorstore=vectorstore,
    retriever=retriever,
    llm=llm,
    guardrails_config=GuardrailsConfig(
        score_threshold=0.4,
        min_sources=2,
        min_avg_score=0.5,
    )
)

# Ingest documents
pipeline.ingest_directory("./documents", file_types=[".pdf", ".txt"])

# Query with confidence
response = pipeline.query("What is machine learning?")
print(f"{response.confidence_emoji} {response.confidence}")
print(f"Answer: {response.answer}")
print(f"Sources: {len(response.sources)}")
```

### REST API
```bash
# Health check
curl http://localhost:8000/health

# Ingest file
curl -X POST http://localhost:8000/ingest/file \
  -H "Content-Type: application/json" \
  -d '{"file_path": "data/sample/document.pdf"}'

# Query
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is gradient descent?", "top_k": 5}'
```

## ğŸ›¡ï¸ Guardrails

The system prevents hallucinations with multiple layers:

| Guard | Default | Description |
|-------|---------|-------------|
| Score Threshold | 0.4 | Min similarity score for chunks |
| Min Sources | 2 | Required quality sources |
| Min Avg Score | 0.5 | Average relevance threshold |

Response confidence levels:
- ğŸŸ¢ **HIGH**: 3+ sources, avg score â‰¥ 0.7
- ğŸŸ¡ **MEDIUM**: 2+ sources, avg score â‰¥ 0.5
- ğŸ”´ **LOW**: Below thresholds (returns uncertainty)

## ğŸ“Š Evaluation
```python
from src.evaluation import RAGEvaluator

evaluator = RAGEvaluator(llm)
result = evaluator.evaluate(
    query="What is ML?",
    answer="Machine learning is...",
    contexts=["ML is a subset of AI..."]
)

print(f"Faithfulness: {result.faithfulness:.2f}")
print(f"Relevance: {result.relevance:.2f}")
print(f"Overall: {result.overall_score:.2f}")
```

## ğŸ§ª Testing
```bash
# Run all tests
python -m pytest tests/unit/ -v

# Run with coverage
python -m pytest tests/unit/ --cov=src --cov-report=html
```

## ğŸ“ Project Structure
```
rag-system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/           # Config, secrets management
â”‚   â”œâ”€â”€ loaders/        # PDF, TXT, MD loaders
â”‚   â”œâ”€â”€ chunkers/       # Fixed, recursive chunkers
â”‚   â”œâ”€â”€ embeddings/     # Ollama embeddings
â”‚   â”œâ”€â”€ vectorstores/   # Qdrant integration
â”‚   â”œâ”€â”€ retrieval/      # Dense retriever
â”‚   â”œâ”€â”€ generation/     # Ollama LLM
â”‚   â”œâ”€â”€ guardrails/     # Production safety
â”‚   â”œâ”€â”€ pipeline/       # RAG orchestration
â”‚   â”œâ”€â”€ evaluation/     # RAGAS metrics
â”‚   â”œâ”€â”€ api/            # FastAPI endpoints
â”‚   â””â”€â”€ ui/             # Streamlit interface
â”œâ”€â”€ tests/
â”œâ”€â”€ config/
â”œâ”€â”€ data/
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

## ğŸ”§ Configuration

Environment variables:
```bash
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest
EMBEDDING_MODEL=nomic-embed-text
QDRANT_HOST=localhost
QDRANT_PORT=6333
COLLECTION_NAME=rag_production
SCORE_THRESHOLD=0.4
MIN_SOURCES=2
MIN_AVG_SCORE=0.5
```

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) - Local LLM inference
- [Qdrant](https://qdrant.tech/) - Vector database
- [FastAPI](https://fastapi.tiangolo.com/) - API framework
- [Streamlit](https://streamlit.io/) - Web UI
- [RAGAS](https://github.com/explodinggradients/ragas) - Evaluation inspiration
