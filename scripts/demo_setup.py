#!/usr/bin/env python3
"""
Demo Setup Script
=================
Run this BEFORE recording to ensure everything is warmed up
and results are consistent.
"""

import time
from src.embeddings import OllamaEmbeddings, CachedEmbeddings
from src.vectorstores.qdrant_hybrid_store import QdrantHybridStore
from src.retrieval import HybridRetriever
from src.generation.ollama_llm import OllamaLLM
from src.documents import MultiDocumentPipeline

def main():
    print("=" * 60)
    print("ğŸ¬ DEMO SETUP - Warming up for recording")
    print("=" * 60)
    
    # Initialize
    print("\n1ï¸âƒ£  Initializing components...")
    embeddings = CachedEmbeddings(
        OllamaEmbeddings(model="nomic-embed-text"), 
        enabled=True
    )
    vectorstore = QdrantHybridStore(
        collection_name="demo_video",
        dense_dimensions=768,
        recreate_collection=True,
    )
    retriever = HybridRetriever(
        embeddings=embeddings,
        vectorstore=vectorstore,
        sparse_encoder="fastembed",
    )
    llm = OllamaLLM(model="llama3.2")
    
    pipeline = MultiDocumentPipeline(
        embeddings=embeddings,
        vectorstore=vectorstore,
        retriever=retriever,
        llm=llm,
        registry_path=".cache/demo_video.json",
    )
    print("   âœ… Components ready")
    
    # Ingest
    print("\n2ï¸âƒ£  Ingesting SEC filings...")
    start = time.time()
    stats = pipeline.ingest_directory(
        "data/test_adaptive/sec-edgar-filings/",
        recursive=True,
    )
    ingest_time = time.time() - start
    print(f"   âœ… {len(stats['companies'])} companies, {stats['total_chunks']} chunks")
    print(f"   â±ï¸  Time: {ingest_time:.1f}s")
    
    # Warm up queries (these will be cached)
    print("\n3ï¸âƒ£  Warming up demo queries...")
    
    demo_queries = [
        ("What factors affect Meta's advertising revenue?", ["Meta"]),
        ("What are Tesla's manufacturing and supply chain risks?", ["Tesla"]),
        ("What are NVIDIA's key products and growth drivers?", ["NVIDIA"]),
        ("What cybersecurity risks do these companies face?", None),
    ]
    
    for query, filter_co in demo_queries:
        start = time.time()
        response = pipeline.query(query, top_k=5, filter_companies=filter_co)
        qtime = time.time() - start
        
        company_str = filter_co[0] if filter_co else "All"
        print(f"   [{company_str}] {response.confidence_emoji} {qtime:.2f}s - {query[:40]}...")
    
    # Warm up comparison
    print("\n4ï¸âƒ£  Warming up comparison query...")
    start = time.time()
    comparison = pipeline.compare_companies(
        "What is the company's AI and machine learning strategy?",
        companies=["Meta", "Tesla", "NVIDIA"],
        top_k_per_company=2,
    )
    ctime = time.time() - start
    print(f"   âœ… Comparison ready: {ctime:.2f}s")
    
    # Stats
    print("\n" + "=" * 60)
    print("âœ… DEMO READY!")
    print("=" * 60)
    print(f"""
Cache stats: {embeddings.stats}

Expected results during demo:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ingestion:      ~2.3 seconds (531 chunks)                   â”‚
â”‚ Meta query:     ~2-3 seconds, HIGH confidence               â”‚
â”‚ Tesla query:    ~2-3 seconds, MEDIUM confidence             â”‚
â”‚ NVIDIA query:   ~2-3 seconds, HIGH confidence               â”‚
â”‚ Comparison:     ~3-4 seconds                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Now open: streamlit run src/ui/app.py
Collection to use: demo_video
""")


if __name__ == "__main__":
    main()
